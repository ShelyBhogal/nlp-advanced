{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf69b1a",
   "metadata": {},
   "source": [
    "# TextRank Summarization method\n",
    "\n",
    "**Using the same dataset of BBC News articles, you can use TextRank methodology on a text document to create a summary.**\n",
    "\n",
    "**Starting with tokenization and vectorization, you apply more advanced calculations to the TF-IDF matrix that require knowledge of linear algebra and the Markov model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf865b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "# For tokenization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# For vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24177e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97b843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bbc_text_cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a98bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b3bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to wrap large text documents nicely\n",
    "\n",
    "def wrap(x):\n",
    "    return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8115d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random business article\n",
    "\n",
    "sample_doc = df[df.labels == 'business']['text'].sample(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7274ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad firm WPP's profits surge 15%\n",
      "\n",
      "UK advertising giant WPP has posted\n",
      "larger-than-expected annual profits and predicted that it will\n",
      "outperform the market in 2005.\n",
      "\n",
      "Pre-tax profits rose 15% from a year\n",
      "ago to reach £546m ($1.04bn), ahead of average analysts' forecasts of\n",
      "£532m.  Revenues were £4.3bn while the firm's operating margins were\n",
      "14.1%, which it said could reach 14.8% by 2006. During the year WPP\n",
      "bought US rival Grey Global, creating a giant big enough to rival\n",
      "sector leader Omnicom.\n",
      "\n",
      "Chief Executive Martin Sorrell on Friday told\n",
      "Reuters news agency that WPP had submitted a proposal for United\n",
      "Business Media's NOP World market research unit.  Analysts say the\n",
      "unit sell could sell for up to £350m.  WPP in recent years has also\n",
      "bought firms such as Ogilvy & Mather and Cordiant Communications.  It\n",
      "also includes the firms Young & Rubicam and J Walter Thompson.  Events\n",
      "such as the Olympics helped boost WPP's profits in 2004. The company\n",
      "said the US Congressional elections and the FIFA World Cup are likely\n",
      "to present advertising opportunities in the near future.  The long-\n",
      "term outlook looks \"very favourable\" because of media and technology\n",
      "developments and the strength of the US economy, WPP said.\n"
     ]
    }
   ],
   "source": [
    "# Wrap entire article (title and body)\n",
    "\n",
    "print(wrap(sample_doc.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0491342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UK advertising giant WPP has posted larger-than-expected annual profits and predicted that it will outperform the market in 2005.\n",
      "\n",
      "Pre-tax profits rose 15% from a year ago to reach £546m ($1.04bn), ahead of average analysts' forecasts of £532m. Revenues were £4.3bn while the firm's operating margins were 14.1%, which it said could reach 14.8% by 2006. During the year WPP bought US rival Grey Global, creating a giant big enough to rival sector leader Omnicom.\n",
      "\n",
      "Chief Executive Martin Sorrell on Friday told Reuters news agency that WPP had submitted a proposal for United Business Media's NOP World market research unit. Analysts say the unit sell could sell for up to £350m. WPP in recent years has also bought firms such as Ogilvy & Mather and Cordiant Communications. It also includes the firms Young & Rubicam and J Walter Thompson. Events such as the Olympics helped boost WPP's profits in 2004. The company said the US Congressional elections and the FIFA World Cup are likely to present advertising opportunities in the near future. The long-term outlook looks \"very favourable\" because of media and technology developments and the strength of the US economy, WPP said.\n"
     ]
    }
   ],
   "source": [
    "# View article body only using split function - 3 paragraphs\n",
    "\n",
    "print(sample_doc.iloc[0].split(\"\\n\", 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454b2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize article body into sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(sample_doc.iloc[0].split(\"\\n\", 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ad4600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nUK advertising giant WPP has posted larger-than-expected annual profits and predicted that it will outperform the market in 2005.',\n",
       " \"Pre-tax profits rose 15% from a year ago to reach £546m ($1.04bn), ahead of average analysts' forecasts of £532m.\",\n",
       " \"Revenues were £4.3bn while the firm's operating margins were 14.1%, which it said could reach 14.8% by 2006.\",\n",
       " 'During the year WPP bought US rival Grey Global, creating a giant big enough to rival sector leader Omnicom.',\n",
       " \"Chief Executive Martin Sorrell on Friday told Reuters news agency that WPP had submitted a proposal for United Business Media's NOP World market research unit.\",\n",
       " 'Analysts say the unit sell could sell for up to £350m.',\n",
       " 'WPP in recent years has also bought firms such as Ogilvy & Mather and Cordiant Communications.',\n",
       " 'It also includes the firms Young & Rubicam and J Walter Thompson.',\n",
       " \"Events such as the Olympics helped boost WPP's profits in 2004.\",\n",
       " 'The company said the US Congressional elections and the FIFA World Cup are likely to present advertising opportunities in the near future.',\n",
       " 'The long-term outlook looks \"very favourable\" because of media and technology developments and the strength of the US economy, WPP said.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475e44fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab99ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize sentences \n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words=stopwords.words('english'), norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2923e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_vect.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "552b70ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11x105 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 129 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse matrix with TF-IDF scores (11 sentences x 105 words)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9296b0",
   "metadata": {},
   "source": [
    "## 1) Calculate cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b91528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between each sentence and every other sentence\n",
    "\n",
    "S = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11406151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for 11-x-11 matrix\n",
    "\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb98a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that there are 11 sentences\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374852d",
   "metadata": {},
   "source": [
    "## 2) Convert to probabilities\n",
    "\n",
    "**In order to create the 'Markov' state transition matrix, you 'normalize' the cosine similarities between each and every sentence by dividing each row by its sum (row values should add up to one).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8843d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure keepdims=True to maintain 2D matrix\n",
    "\n",
    "S /= S.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc85af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that rows add up to one\n",
    "\n",
    "S[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b2f35",
   "metadata": {},
   "source": [
    "**Apply smoothing to the Markov matrix to deal with zero values, by creating a 'uniform' matrix of ones divided by the number of sentences, and combining it with the Markov matrix.**\n",
    "\n",
    "**The 'uniform' matrix by multiplying by a factor of 0.15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98601be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uniform transition matrix in same shape as sentences\n",
    "\n",
    "U = np.ones_like(S) / len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce16a320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows should add up to one\n",
    "\n",
    "U[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01934a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Smoothed' matrix - convex combination of S and U\n",
    "\n",
    "factor = 0.15\n",
    "\n",
    "S = (1 - factor) * S + factor * U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f885af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows should add up to one\n",
    "\n",
    "S[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f73f7a",
   "metadata": {},
   "source": [
    "## 3) Find the limiting distribution\n",
    "\n",
    "**Also referred to as the stationary distribution, the limiting distribution is basically the scores for each sentence, and you need to compute the eigenvalues and eigenvectors of the 'smoothed' probability matrix**\n",
    "\n",
    "**Eigenvalues** are a set of special scalar values that represent importance, associated with a linear system.\n",
    "\n",
    "**Eigenvectors** are a set of special vectors associated with a linear system.\n",
    "\n",
    "Since eigenvalue equations are done in column vectors, not row vectors, you must transpose the Markov matrix before finding the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82544bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the eigenvalues and eigenvectors from transposed matrix\n",
    "\n",
    "eigenvals, eigenvecs = np.linalg.eig(S.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2b15e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.79424305, 0.74403335, 0.49082468, 0.52102797,\n",
       "       0.70229786, 0.67039834, 0.61843748, 0.57085743, 0.59890148,\n",
       "       0.58373726])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 eigenvalues (one should be 1)\n",
    "\n",
    "eigenvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "504d7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 eigenvectors\n",
    "\n",
    "len(eigenvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "255ea2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31540874, -0.29916867, -0.2934562 , -0.30751681, -0.30162485,\n",
       "       -0.29140274, -0.31760331, -0.2852635 , -0.29789447, -0.29632761,\n",
       "       -0.30925864])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st eigenvector (1st column) - do not worry about negative values yet\n",
    "\n",
    "eigenvecs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a12bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31540874, -0.29916867, -0.2934562 , -0.30751681, -0.30162485,\n",
       "       -0.29140274, -0.31760331, -0.2852635 , -0.29789447, -0.29632761,\n",
       "       -0.30925864])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test assumption that this is an eigenvector - note that values do not change\n",
    "\n",
    "eigenvecs[:, 0].dot(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeb625a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09514806, 0.09024899, 0.08852573, 0.09276734, 0.09098993,\n",
       "       0.08790627, 0.09581009, 0.08605427, 0.08986461, 0.08939194,\n",
       "       0.09329279])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make all values positive and add up to one, divided vector by its total \n",
    "\n",
    "eigenvecs[:, 0] / eigenvecs[:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9595678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- BRUTE FORCE METHOD TO FIND LIMITING DISTRIBUTION ------------------------- #\n",
    "\n",
    "\n",
    "# Initialize limiting distribution matrix\n",
    "limiting_dist = np.ones(len(S)) / len(S)\n",
    "\n",
    "# Define threshold (10 to the power of -8)\n",
    "threshold = 1e-8\n",
    "\n",
    "# Stores how much distribution changes from one step to next\n",
    "delta = float('inf')\n",
    "\n",
    "# Store current iteration number (counter)\n",
    "iters = 0\n",
    "\n",
    "while delta > threshold:\n",
    "    iters += 1 \n",
    "    \n",
    "    # Same as Markov matrix\n",
    "    p = limiting_dist.dot(S) \n",
    "    \n",
    "    # Compute change in limiting distribution \n",
    "    delta = np.abs(p - limiting_dist).sum() \n",
    "    \n",
    "    # Update limiting distribution \n",
    "    limiting_dist = p\n",
    "\n",
    "print(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ce74d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09514806, 0.09024899, 0.08852574, 0.09276733, 0.09098993,\n",
       "       0.08790628, 0.09581008, 0.08605426, 0.0898646 , 0.08939194,\n",
       "       0.09329278])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that these values match eigenvecs[:, 0] / eigenvecs[:, 0].sum()\n",
    "\n",
    "limiting_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c76e8002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values should add up to one\n",
    "\n",
    "limiting_dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "708d29a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.496053004037325e-08"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate absolute difference between limiting distribution and 1st eigenvector divided by itself\n",
    "\n",
    "np.abs(eigenvecs[:,0] / eigenvecs[:,0].sum() - limiting_dist).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118dee1",
   "metadata": {},
   "source": [
    "**The limiting distribution is the same as the first eigenvector divided by its total:**\n",
    "\n",
    "`limiting_dist = eigenvecs[:, 0] / eigenvecs[:, 0].sum()`\n",
    "\n",
    "**If you calculate the difference between the two, as above, the answer is approx 0.000000035, which is basically zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8eacd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the limiting distribution to scores and sort index numbers in descending order\n",
    "\n",
    "scores = limiting_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df02d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4350d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  0, 10,  3,  4,  1,  8,  9,  2,  5,  7], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841a975",
   "metadata": {},
   "source": [
    "### There are many options for how to choose which sentences to include:\n",
    "\n",
    "* top N sentences\n",
    "* top N words\n",
    "* top X% sentences or top X% words\n",
    "* sentences with scores > average score\n",
    "* sentences with scores > factor * average score\n",
    "\n",
    "**You also don't have to sort, as it may make more sense in the same order as in the original document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0fd06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary:\n",
      "\n",
      "0.10: WPP in recent years has also bought firms such as Ogilvy &\n",
      "Mather and Cordiant Communications.\n",
      "0.10: \n",
      "UK advertising giant WPP has posted larger-than-expected annual\n",
      "profits and predicted that it will outperform the market in 2005.\n",
      "0.09: The long-term outlook looks \"very favourable\" because of media\n",
      "and technology developments and the strength of the US economy, WPP\n",
      "said.\n",
      "0.09: During the year WPP bought US rival Grey Global, creating a\n",
      "giant big enough to rival sector leader Omnicom.\n",
      "0.09: Chief Executive Martin Sorrell on Friday told Reuters news\n",
      "agency that WPP had submitted a proposal for United Business Media's\n",
      "NOP World market research unit.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated summary:\\n\")\n",
    "\n",
    "for i in sort_idx[:5]:\n",
    "    print(wrap(\"%.2f: %s\" % (scores[i], sentences[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efcd0b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ad firm WPP's profits surge 15%\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View original article title\n",
    "\n",
    "sample_doc.iloc[0].split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18145cd2",
   "metadata": {},
   "source": [
    "## Create TextRank summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f86a98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to TextRank document - all steps from tokenization to printing summary\n",
    "\n",
    "def summary(text, factor = 0.15):\n",
    "    # Extract sentences \n",
    "    sents = nltk.sent_tokenize(text) \n",
    "    \n",
    "    # Perform TF-IDF\n",
    "    featurizer = TfidfVectorizer(stop_words=stopwords.words('english'), norm='l1') \n",
    "    X = featurizer.fit_transform(sents) \n",
    "    \n",
    "    # Compute cosine similarity \n",
    "    S = cosine_similarity(X) \n",
    "    \n",
    "    # Normalize cosine similarity matrix \n",
    "    S /= S.sum(axis=1, keepdims=True) \n",
    "    \n",
    "    # Uniform transition matrix \n",
    "    U = np.ones_like(S) / len(S) \n",
    "    \n",
    "    # Smoothed similarity matrix \n",
    "    S = (1 - factor) * S + factor * U \n",
    "    \n",
    "    # Find the limiting / stationary distribution \n",
    "    eigenvals, eigenvecs = np.linalg.eig(S.T) \n",
    "    \n",
    "    # Compute scores \n",
    "    scores = eigenvecs[:,0] / eigenvecs[:,0].sum() \n",
    "    \n",
    "    # Sort the scores \n",
    "    sort_idx = np.argsort(-scores) \n",
    "    \n",
    "    # Print top five sentences with scores \n",
    "    for i in sort_idx[:5]:\n",
    "        print(wrap(\"%.2f: %s\" % (scores[i], sents[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fa87b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function on entertainment article\n",
    "\n",
    "doc = df[df.labels == 'entertainment']['text'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba75f655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12: Rapper Young Buck has been charged after allegedly stabbing a\n",
      "man who hit Dr Dre as he was about to receive a lifetime achievement\n",
      "award.\n",
      "0.12: Mr Johnson allegedly approached Dr Dre, who was seated at a\n",
      "table in front of the stage, and appeared to ask for an autograph\n",
      "before punching him.\n",
      "0.11: He said not holding the awards would be counter to the work the\n",
      "magazine has done to promote hip hop music.\n",
      "0.11: Vibe magazine president Kenard Gibbs said the attack earlier\n",
      "this month in Santa Monica was \"sickening\".\n",
      "0.11: \n",
      "The US Vibe awards will be held again next year despite a\n",
      "stabbing which happened during the ceremony.\n"
     ]
    }
   ],
   "source": [
    "summary(doc.iloc[0].split(\"\\n\", 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89a6a19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vibe awards back despite violence'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View article title\n",
    "\n",
    "doc.iloc[0].split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6c241",
   "metadata": {},
   "source": [
    "# TextRank summarization with Sumy\n",
    "\n",
    "You must first install Sumy with Windows command terminal:\n",
    "\n",
    "    pip install sumy\n",
    "    pip install git+git://github.com/miso-belica/sumy.git\n",
    "\n",
    "Then you can import the necessary modules to use on the same randomly-selected entertainment article above, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aed37808",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sumy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msumy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sumy'"
     ]
    }
   ],
   "source": [
    "import sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f2ea777",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sumy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msumy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_rank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextRankSummarizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msumy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlsa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LsaSummarizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msumy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplaintext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PlaintextParser\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sumy'"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TextRank Summarizer\n",
    "\n",
    "summarizer = TextRankSummarizer()\n",
    "\n",
    "parser = PlaintextParser.from_string(doc.iloc[0].split(\"\\n\", 1)[1], Tokenizer(\"english\"))\n",
    "\n",
    "tr_summary = summarizer(parser.document, sentences_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8288823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through sentences in summary to wrap each in output\n",
    "\n",
    "for s in tr_summary:\n",
    "    print(wrap(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LSA Summarizer\n",
    "\n",
    "summarizer = LsaSummarizer()\n",
    "\n",
    "lsa_summary = summarizer(parser.document, sentences_count=5)\n",
    "\n",
    "for s in lsa_summary:\n",
    "    print(wrap(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN'T INSTALL SUMY...!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5154259",
   "metadata": {},
   "source": [
    "# Text summarization with Gensim\n",
    "\n",
    "The parameters for Gensim's **`summarize()`** function are:\n",
    "    \n",
    "* **`text`** (string) – given text document\n",
    "* **`ratio`** (float, optional) – number between 0 and 1 that determines the proportion of the number of sentences of the original text to be chosen for the summary, e.g. 10%, 20%.\n",
    "* **`word_count`** (int or None, optional) – determines how many words will be contained in the output. If both `ratio` and `word_count` parameters are specified, the ratio will be ignored.\n",
    "* **`split`** (bool, optional) – if True, list of sentences will be returned. Otherwise joined strings will be returned.\n",
    "\n",
    "\n",
    "**Since this lecture, `gensim.summarization` code was removed from version Gensim 4.0. You could install an older version (3.8.3) or wait till a replacement has been implemented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6decd9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[0;32m      3\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize(doc\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(wrap(summary))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "summary = summarize(doc.iloc[0].split(\"\\n\", 1)[1])\n",
    "\n",
    "print(wrap(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec71004",
   "metadata": {},
   "source": [
    "The formal documentation for `summarize()` function can be accessed from link below:\n",
    "\n",
    "https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95e0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
