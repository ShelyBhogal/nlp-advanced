{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad8a67a",
   "metadata": {},
   "source": [
    "# Text Pre-processing in TensorFlow\n",
    "\n",
    "TensorFlow supports tokenization and vectorization of text data, similar to SciKit-Learn, which can come in handy. The most well-known is the **`Tokenizer`** class that accepts a list of strings and converts tokens to integers (as well as tokenizing), so that you end up with documents that are sequences of integers.\n",
    "\n",
    "It is also worth noting the **`pad_sequences`** function, that pads out sequences of integers so that they are all of equal length, and can also perform other operations on the tokenized sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75aed7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d10dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc4309",
   "metadata": {},
   "source": [
    "**Lets look at simple example of using the `Tokenizer`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50533c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I like eggs and ham.\", \n",
    "             \"I love chocolate and bunnies.\", \n",
    "             \"I hate onions.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34419ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max vocabulary size is usually this large\n",
    "max_vocab = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "\n",
    "# Fit the tokenizer on sentences\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b669e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sentences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d719d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# Integers start from 1, not 0\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bce3c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'and': 2,\n",
       " 'like': 3,\n",
       " 'eggs': 4,\n",
       " 'ham': 5,\n",
       " 'love': 6,\n",
       " 'chocolate': 7,\n",
       " 'bunnies': 8,\n",
       " 'hate': 9,\n",
       " 'onions': 10}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word-to-index mapping generated through tokenizer\n",
    "\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b096112",
   "metadata": {},
   "source": [
    "**Lets look at a simple example using `pad_sequences` function, which 'pads' out the sequences so that each vector is of the same length:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23838b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2  5]\n",
      " [ 1  6  7  2  8]\n",
      " [ 0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f359a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  3  4  2  5]\n",
      " [ 0  1  6  7  2  8]\n",
      " [ 0  0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "# You can increase the length of each sequence when padding (although no point!)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=6)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65bafa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2  5]\n",
      " [ 1  6  7  2  8]\n",
      " [ 1  9 10  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# You can pad the sequences at the end \n",
    "\n",
    "data = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d62a3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  2  5]\n",
      " [ 6  7  2  8]\n",
      " [ 0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "# You can truncate the sequences by setting lower max length - takes from beginning\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=4)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f8524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
