{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad8a67a",
   "metadata": {},
   "source": [
    "# Text Pre-processing in TensorFlow\n",
    "\n",
    "TensorFlow supports tokenization and vectorization of text data, similar to SciKit-Learn, which can come in handy. The most well-known is the **`Tokenizer`** class that accepts a list of strings and converts tokens to integers (as well as tokenizing).\n",
    "\n",
    "It is also worth noting the **`pad_sequences`** function, that pads out sequences of integers so that they are all of equal length, and can also perform other operations on the tokenized sequences.\n",
    "\n",
    "\n",
    "\n",
    "The dataset you will be using is a useful benchmark datasets for NLP tasks from *NLP-progress*. The dataset **Text8** was prepared by taking the 'XML dump' (archived WikiPedia articles) from March 2006 and retaining only the text that you would have read on a Wikipedia page. The dataset is usually treated as one long sequence and consists of only lowercase English characters and spaces.\n",
    "\n",
    "In order to download the dataset, you need to import Gensim module **`gensim.downloader`**, which is an API for downloading datasets for language modeling exercises.\n",
    "\n",
    "If youâ€™re interested in the exact processing of the 'XML dump', you can see online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75aed7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d10dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0030a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the text8 dataset - ALL THIS NOT NEEDED!!\n",
    "\n",
    "#import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e933deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NEEDED\n",
    "\n",
    "#dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d49c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE!!\n",
    "\n",
    "#type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc4309",
   "metadata": {},
   "source": [
    "**Lets look at simple example of using the `Tokenizer`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50533c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I like eggs and ham.\", \n",
    "             \"I love chocolate and bunnies.\", \n",
    "             \"I hate onions.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34419ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max vocabulary size is usually this large\n",
    "max_vocab = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "\n",
    "# Fit the tokenizer on sentences\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b669e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sentences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d719d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# Integers start from 1, not 0\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bce3c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'and': 2,\n",
       " 'like': 3,\n",
       " 'eggs': 4,\n",
       " 'ham': 5,\n",
       " 'love': 6,\n",
       " 'chocolate': 7,\n",
       " 'bunnies': 8,\n",
       " 'hate': 9,\n",
       " 'onions': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word-to-index mapping generated through tokenizer\n",
    "\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b096112",
   "metadata": {},
   "source": [
    "**Lets look at a simple example using `pad_sequences` function, which 'pads' out the sequences so that each vector is of the same length:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23838b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4  2  5]\n",
      " [ 1  6  7  2  8]\n",
      " [ 0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f359a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  3  4  2  5]\n",
      " [ 0  1  6  7  2  8]\n",
      " [ 0  0  0  1  9 10]]\n"
     ]
    }
   ],
   "source": [
    "# You can increase the length of each sequence when padding (although no point!)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=6)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bafa32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
